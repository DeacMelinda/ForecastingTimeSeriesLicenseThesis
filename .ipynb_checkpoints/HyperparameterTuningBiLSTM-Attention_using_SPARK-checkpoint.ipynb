{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd322e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, SparkTrials, STATUS_FAIL\n",
    "from hyperopt.pyll.base import scope\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from attention import Attention\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "conf = SparkConf().setAppName(\"Hyperopt with Spark\") \\\n",
    "                  .setMaster(\"local[*]\") \\\n",
    "                  .set(\"spark.driver.memory\", \"8g\") \\\n",
    "                  .set(\"spark.executor.memory\", \"8g\") \\\n",
    "                  .set(\"spark.executor.cores\", \"4\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from a file\n",
    "def read_data(file_location):\n",
    "    df = pd.read_csv(file_location)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a66424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_one_day(pandas_df_to_plot,year_to_plot,month_to_plot, day_to_plot):\n",
    "    p_one_day = pandas_df_to_plot[(pandas_df_to_plot.index.day == day_to_plot) & (pandas_df_to_plot.index.year == year_to_plot) & (pandas_df_to_plot.index.month == month_to_plot)]\n",
    "    return p_one_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb82505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_one_month(pandas_df_to_plot,year_to_plot,month_to_plot, day_to_plot):\n",
    "    p_one_day = pandas_df_to_plot[(pandas_df_to_plot.index.day == day_to_plot) & (pandas_df_to_plot.index.year == year_to_plot) & (pandas_df_to_plot.index.month == month_to_plot)]\n",
    "    return p_one_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34972e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_day(pandas_df_to_plot,year_to_plot,month_to_plot, day_to_plot):\n",
    "    p_to_plot = df_one_day(pandas_df_to_plot,year_to_plot,month_to_plot, day_to_plot)\n",
    "    plt.rcParams[\"figure.figsize\"] = (40,3)\n",
    "    plt.plot(p_to_plot)\n",
    "    plt.show()\n",
    "    print(\"Plotting: year = \", year_to_plot, \" month = \", month_to_plot, \" day = \", day_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ade68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_month(pandas_df_to_plot,year_to_plot,month_to_plot):\n",
    "    days_from_month = pandas_df_to_plot[(pandas_df_to_plot.index.year == year_to_plot) & (pandas_df_to_plot.index.month == month_to_plot)].index.day.unique().values\n",
    "    for day in days_from_month:\n",
    "        plot_one_day(pandas_df_to_plot, year_to_plot,month_to_plot, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917370d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_max_freq_month(df):\n",
    "    door_open_monthly = df.resample('M').sum()\n",
    "    max_month_year = door_open_monthly[door_open_monthly.value == door_open_monthly.value.max()].index.year[0]\n",
    "    max_month_month = door_open_monthly[door_open_monthly.value == door_open_monthly.value.max()].index.month[0]\n",
    "    print('Year with max door opening:', max_month_year)\n",
    "    print('Month with max door opening:', max_month_month)\n",
    "    plot_one_month(df,max_month_year,max_month_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_months(df, year):\n",
    "    return df[(df.index.year == year)].index.month.unique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3672643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_years(df):\n",
    "    return df.index.year.unique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_year(df, year):\n",
    "    all_months = get_all_months(df, year)\n",
    "    for month in all_months:\n",
    "        plot_one_month(df, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_years(df):\n",
    "    all_years = get_all_years(df)\n",
    "    for year in all_years:\n",
    "        plot_one_year(df, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ba8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and clean data\n",
    "def preprocess_data_with_splits(df, aggregation='H', ws=24, number_of_predicted_days=2):\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df.drop_duplicates(subset=['source_ts'], inplace=True)\n",
    "\n",
    "    datetime_series = pd.to_datetime(df['source_ts'])\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    df=df.set_index(datetime_index)\n",
    "    df.drop('source_ts',axis=1,inplace=True)\n",
    "\n",
    "    df=df.asfreq(freq='S', method='ffill')\n",
    "\n",
    "    lastDay = df.index[-1].strftime('%Y-%m-%d')\n",
    "    df = df.loc[:lastDay].iloc[:-1 , :]\n",
    "    df\n",
    "\n",
    "    prediction_in_future_time = ws * number_of_predicted_days\n",
    "    \n",
    "    df_resampled = df.resample(aggregation).sum()\n",
    "    df_resampled\n",
    "    \n",
    "    df = df_resampled\n",
    "    n_splits = 4\n",
    "    test_size = 48\n",
    "    total_len = len ( df )\n",
    "    fold_size = (total_len - test_size) // n_splits\n",
    "    tscv = TimeSeriesSplit ( n_splits = n_splits)\n",
    "    splits = []\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        test_indices = np.arange(test_index[0], test_index[0] + test_size)\n",
    "        train_indices = np.arange(0, test_indices[0])\n",
    "        splits.append((train_indices[0], train_indices[-1], test_indices[0], test_indices[-1]))\n",
    "\n",
    "    return df_resampled, splits, n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_simple(df, aggregation='H', ws=24, number_of_predicted_days=2):\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df.drop_duplicates(subset=['source_ts'], inplace=True)\n",
    "\n",
    "    datetime_series = pd.to_datetime(df['source_ts'])\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    df=df.set_index(datetime_index)\n",
    "    df.drop('source_ts',axis=1,inplace=True)\n",
    "\n",
    "    df=df.asfreq(freq='S', method='ffill')\n",
    "\n",
    "    lastDay = df.index[-1].strftime('%Y-%m-%d')\n",
    "    df = df.loc[:lastDay].iloc[:-1 , :]\n",
    "    df\n",
    "\n",
    "    prediction_in_future_time = ws * number_of_predicted_days\n",
    "    \n",
    "    df_resampled = df.resample(aggregation).sum()\n",
    "    df_resampled\n",
    "    \n",
    "    year_to_plot = df.index[-1].year\n",
    "    month_to_plot = df.index[-1].month\n",
    "    day_to_plot = df.index[-1].day\n",
    "\n",
    "    plot_one_day(df, year_to_plot, month_to_plot, day_to_plot)\n",
    "\n",
    "    plot_one_day(df_resampled, year_to_plot, month_to_plot, day_to_plot)\n",
    "\n",
    "    df = df_resampled\n",
    "\n",
    "    index_of_start_prediction = 0 - prediction_in_future_time\n",
    "    day_of_start_prediction = df.index[index_of_start_prediction].strftime('%Y-%m-%d')\n",
    "    index_of_end_train = index_of_start_prediction - ws\n",
    "    day_of_end_train = df.index[index_of_end_train].strftime('%Y-%m-%d')\n",
    "\n",
    "    TRAIN_END = day_of_end_train\n",
    "    TEST_START = day_of_start_prediction\n",
    "\n",
    "    training_set_df = df.loc[:TRAIN_END]\n",
    "\n",
    "    test_set_df = df.loc[TEST_START:]\n",
    "    \n",
    "    return df_resampled, TRAIN_END, TEST_START, training_set_df, test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3690b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions using the trained model\n",
    "def make_predictions(model, batch_one, prediction_in_future_time):\n",
    "    prediction_test = []\n",
    "    batch_new = batch_one.reshape((1, ws, 1))\n",
    "    \n",
    "    for _ in range(prediction_in_future_time):\n",
    "        first_pred = model.predict(batch_new)[0]\n",
    "        prediction_test.append(first_pred)\n",
    "        batch_new = np.append(batch_new[:, 1:, :], [[first_pred]], axis=1)\n",
    "\n",
    "    return np.array(prediction_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ed61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(test_set, predictions):\n",
    "    rmse = np.sqrt(mean_squared_error(test_set, predictions))\n",
    "    rsquare = r2_score(test_set, predictions)\n",
    "    return rmse, rsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the actual vs predicted values\n",
    "def plot_results(test_set, predictions):\n",
    "    plt.plot(test_set, color='green', label='Actual value')\n",
    "    plt.plot(predictions, color='orange', label='Predicted value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input sequences for training the LSTM model\n",
    "def create_input_sequences(data, ws):\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    for i in range(ws, len(data)):\n",
    "        x_train.append(data[i-ws:i, 0:1])\n",
    "        y_train.append(data[i, 0])\n",
    "\n",
    "    return np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a84fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for Hyperopt\n",
    "def objective(params):\n",
    "    try:\n",
    "        units = int(params['units'])\n",
    "        num_layers = int(params['num_layers'])\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        epochs = int(params['epochs'])\n",
    "        batch_size = int(params['batch_size'])\n",
    "        attention_units = int(params['attention_units'])\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        for (dataset, nbDays) in datasets:\n",
    "            df = read_data(dataset)\n",
    "    \n",
    "            if(nbDays < 182):\n",
    "                \n",
    "                df_resampled, TRAIN_END, TEST_START, training_set_df, test_set_df = preprocess_data_simple(df)\n",
    "                training_set = training_set_df.values\n",
    "                test_set = test_set_df.values\n",
    "\n",
    "                sc = MinMaxScaler(feature_range=(0, 1))\n",
    "                training_set_scaled = sc.fit_transform(training_set)\n",
    "                test_set_scaled = sc.fit_transform(test_set)\n",
    "                \n",
    "                x_train, y_train = create_input_sequences(training_set_scaled, ws)\n",
    "                shape = x_train.shape[1]\n",
    "                \n",
    "                model = create_model(units, num_layers, dropout_rate, shape, attention_units)\n",
    "                model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "                batch_one = test_set_scaled[-ws:]\n",
    "                prediction_in_future_time = 2*ws\n",
    "                predictions = make_predictions(model, batch_one, prediction_in_future_time)\n",
    "\n",
    "                predictions = sc.inverse_transform(predictions)\n",
    "\n",
    "                rmse, rsquare = evaluate_model(test_set, predictions)\n",
    "                log_message = (f\"For units={units}, num_layers={num_layers}, dropout_rate={dropout_rate}, epochs={epochs}, batch_size={batch_size}, attention_units={attention_units}\\n DATASET: {dataset}\\n RMSE: {rmse}\\n R-squared: {rsquare}\\n\")\n",
    "        \n",
    "                with open(log_file_path, 'a') as f:\n",
    "                    f.write(log_message)\n",
    "                results.append(rmse)\n",
    "                \n",
    "            else:\n",
    "                df_resampled, splits, n_splits = preprocess_data_with_splits(df)\n",
    "\n",
    "                rmse_values = []\n",
    "                r_squared_values = []\n",
    "                for split in range(n_splits):\n",
    "                    train_start, train_end, test_start, test_end = splits[split]\n",
    "                    training_set = df_resampled[train_start:train_end + 1].values\n",
    "                    test_set = df_resampled[test_start:test_end + 1].values\n",
    "                    sc = MinMaxScaler(feature_range=(0,1))\n",
    "                    training_set_scaled = sc.fit_transform(training_set)\n",
    "                    x_train, y_train = create_input_sequences(training_set_scaled, ws)\n",
    "                    shape = x_train.shape[1]\n",
    "                    model = create_model(units, num_layers, dropout_rate, shape, attention_units)\n",
    "                    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "                    batch_one = training_set_scaled[-ws:]\n",
    "                    prediction_in_future_time = test_end - test_start + 1\n",
    "                    prediction = make_predictions(model, batch_one, prediction_in_future_time)\n",
    "\n",
    "                    prediction = sc.inverse_transform(prediction)\n",
    "                    rmse, rsquare = evaluate_model(test_set, prediction)\n",
    "                    rmse_values.append(rmse)\n",
    "                    r_squared_values.append(rsquare)\n",
    "        \n",
    "                log_message = (f\"For units={units}, num_layers={num_layers}, dropout_rate={dropout_rate}, epochs={epochs}, batch_size={batch_size}, attention_units={attention_units}\\n DATASET: {dataset} \\n average RMSE: {np.mean(rmse_values)}\\n average R-squared: {np.mean(r_squared_values)}\\n\")\n",
    "\n",
    "                with open(log_file_path, 'a') as f:\n",
    "                    f.write(log_message)\n",
    "                    \n",
    "                results.append(np.mean(rmse_values))\n",
    "        \n",
    "        return {'loss': (sum(results)/len(results)), 'status': STATUS_OK}\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in objective function: {e}\")\n",
    "        return {'loss': float('inf'), 'status': STATUS_FAIL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "search_space = {\n",
    "    'units': scope.int(hp.choice('units', [32, 64, 128])),\n",
    "    'num_layers': scope.int(hp.choice('num_layers', [3, 4, 5, 6, 10])),\n",
    "    'dropout_rate': hp.choice('dropout_rate', [0.1,0.2,0.3,0.5]),\n",
    "    'attention_units': scope.int(hp.choice('attention_units', [8,16,32,64])),\n",
    "    'epochs': scope.int(hp.choice('epochs', [15, 30, 60])),\n",
    "    'batch_size': scope.int(hp.choice('batch_size', [16, 32, 64]))\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    ('./testData/Dataset1.csv',121),\n",
    "    ('./testData/Dataset2.csv',121),\n",
    "    ('./testData/Dataset3.csv',61),\n",
    "    ('./testData/Dataset4.csv',366),\n",
    "    ('./testData/Dataset5.csv',361),\n",
    "    ('./testData/Dataset6.csv',361),\n",
    "    ('./testData/new/Dataset7.csv',31),\n",
    "    ('./testData/new/Dataset8.csv', 361),\n",
    "    ('./testData/new/Dataset9.csv',61),\n",
    "    ('./testData/new/Dataset10.csv',721),\n",
    "    ('./testData/new/Dataset11.csv',91),\n",
    "    ('./testData/new/Dataset12.csv',359)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d61919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an LSTM model\n",
    "def create_model(units, num_layers, dropout_rate, shape, attention_units):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=units, return_sequences=True), input_shape=(shape, 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for _ in range(num_layers - 2):\n",
    "        model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    model.add(Attention(attention_units))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "\n",
    "    spark_trials = SparkTrials(parallelism=4)\n",
    "    best_params = fmin(\n",
    "        fn=objective, \n",
    "        space=search_space, \n",
    "        algo=tpe.suggest, \n",
    "        max_evals=15,\n",
    "        trials = spark_trials)\n",
    "    \n",
    "    best_model = sorted(trials.results, key=lambda result: result[\"loss\"])[0][\"loss\"]\n",
    "    print(\"Best parameters: \", best_params)\n",
    "    \n",
    "    print(\"Best model: \", best_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b57583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws = 24\n",
    "log_file_path = \"Results.txt\"\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
